{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "* https://docs.chainer.org/en/stable/tutorial/basic.html\n",
    "* https://docs.chainer.org/en/stable/tutorial/gpu.html\n",
    "\n",
    "## See Also:\n",
    "* https://github.com/hido/chainer-handson/blob/master/chainer.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad INIT None\n",
      "y.data [ 16.]\n",
      "x.grad [ 8.]\n",
      "z.grad [-1.]\n",
      "y.data [[  0.   1.   4.]\n",
      " [  9.  16.  25.]]\n",
      "x.grad [[  0.   2.   4.]\n",
      " [  6.   8.  10.]]\n"
     ]
    }
   ],
   "source": [
    "# Forward/Backward Computation\n",
    "x_data = np.array([5], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "print('x.grad INIT',x.grad)\n",
    "\n",
    "y = x**2 - 2 * x + 1\n",
    "print('y.data',y.data)\n",
    "\n",
    "#y.grad = np.ones((1,), dtype=np.float32) # Not needed since dim=1,\n",
    "y.backward()\n",
    "print('x.grad',x.grad) # == 2*x - 2\n",
    "\n",
    "z = 2*x\n",
    "y = x**2 - z + 1\n",
    "y.backward(retain_grad=True)\n",
    "print('z.grad',z.grad)\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32))\n",
    "y = x**2 - 2*x + 1\n",
    "y.grad = np.ones((2, 3), dtype=np.float32) # Needed since dim!=1,\n",
    "y.backward()\n",
    "print('y.data',y.data)\n",
    "print('x.grad',x.grad) # == 2*x - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f.W.data [[-0.25846836  0.14406465  0.23706345]\n",
      " [-0.26762658  0.23766576 -1.45790076]]\n",
      "f.b.data [ 0.  0.]\n",
      "y.data [[ 0.74085128 -4.16599703]\n",
      " [ 1.10883045 -8.62958145]]\n",
      "f.W.grad [[ 5.  7.  9.]\n",
      " [ 5.  7.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# Links\n",
    "f = L.Linear(3, 2)\n",
    "print('f.W.data',f.W.data)\n",
    "print('f.b.data',f.b.data)\n",
    "\n",
    "x = Variable(np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32))\n",
    "y = f(x)\n",
    "print('y.data',y.data)\n",
    "\n",
    "f.cleargrads() # if missing then grad will be NaN\n",
    "# print('f.W.grad',f.W.grad) # after cleargrads(), set to None\n",
    "y.grad = np.ones((2, 2), dtype=np.float32)\n",
    "y.backward()\n",
    "print('f.W.grad',f.W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a model as a chain\n",
    "class MyChain(Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(4, 3)\n",
    "            self.l2 = L.Linear(3, 2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.l1(x)\n",
    "        return self.l2(h)\n",
    "\n",
    "class MyChain2(ChainList):\n",
    "    def __init__(self):\n",
    "        super(MyChain2, self).__init__(\n",
    "            L.Linear(4, 3),\n",
    "            L.Linear(3, 2),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self[0](x)\n",
    "        return self[1](h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.l1.W.data [[ 0.26474601  0.65291721 -1.13191652  0.16819128]\n",
      " [ 0.25721487  0.63281906  0.47706214  1.12798083]\n",
      " [-0.03506165 -0.00135101 -0.40000299 -0.14114116]]\n",
      "model.l1.b.data [ 0.  0.  0.]\n",
      "AFTER OPTIMIZER UPDATE\n",
      "model.l1.W.data [[ 0.26232165  0.65381378 -1.13263738  0.16930406]\n",
      " [ 0.28351146  0.62304968  0.48494422  1.11588883]\n",
      " [-0.05419634  0.00575511 -0.40573788 -0.13234614]]\n",
      "model.l1.b.data [ 0.00210259 -0.02281998  0.01660428]\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "model = MyChain()\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(0.0005))\n",
    "print('model.l1.W.data',model.l1.W.data)\n",
    "print('model.l1.b.data',model.l1.b.data)\n",
    "\n",
    "# Manually call optimizer.update using lossfun\n",
    "def lossfun(arg1, arg2):\n",
    "        # calculate loss\n",
    "        loss = F.sum(model(arg1 - arg2))\n",
    "        return loss\n",
    "arg1 = np.random.uniform(-1, 1, (2, 4)).astype('f')\n",
    "arg2 = np.random.uniform(-1, 1, (2, 4)).astype('f')\n",
    "optimizer.update(lossfun, chainer.Variable(arg1), chainer.Variable(arg2))\n",
    "print('AFTER OPTIMIZER UPDATE')\n",
    "print('model.l1.W.data',model.l1.W.data)\n",
    "print('model.l1.b.data',model.l1.b.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/accuracy  validation/main/accuracy\n",
      "\u001b[J1           0.719817       0.8654                    \n",
      "\u001b[J2           0.877417       0.8952                    \n",
      "\u001b[J3           0.8968         0.9054                    \n",
      "\u001b[J4           0.905267       0.9143                    \n",
      "\u001b[J5           0.910817       0.9184                    \n",
      "\u001b[J6           0.9162         0.9219                    \n",
      "\u001b[J7           0.920033       0.9241                    \n",
      "\u001b[J8           0.923667       0.9283                    \n",
      "\u001b[J9           0.927267       0.9318                    \n",
      "\u001b[J10          0.930367       0.9339                    \n",
      "\u001b[J11          0.933533       0.935                     \n",
      "\u001b[J12          0.936183       0.9391                    \n",
      "\u001b[J13          0.9385         0.9403                    \n",
      "\u001b[J14          0.9405         0.9413                    \n",
      "\u001b[J15          0.94285        0.9417                    \n",
      "\u001b[J16          0.94495        0.9457                    \n",
      "\u001b[J17          0.946717       0.9466                    \n",
      "\u001b[J18          0.948183       0.9481                    \n",
      "\u001b[J19          0.949717       0.9491                    \n",
      "\u001b[J20          0.951467       0.9512                    \n"
     ]
    }
   ],
   "source": [
    "# Example: Multi-layer Perceptron on MNIST\n",
    "train, test = datasets.get_mnist()\n",
    "train_iter = iterators.SerialIterator(train, batch_size=100, shuffle=True)\n",
    "test_iter = iterators.SerialIterator(test, batch_size=100, repeat=False, shuffle=False)\n",
    "\n",
    "class MLP(Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.leaky_relu(self.l1(x), slope=0.1)\n",
    "        h2 = F.leaky_relu(self.l2(h1), slope=0.1)\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "\n",
    "class Classifier(Chain):\n",
    "    def __init__(self, predictor):\n",
    "        super(Classifier, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.predictor = predictor\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        accuracy = F.accuracy(y, t)\n",
    "        report({'loss': loss, 'accuracy': accuracy}, self)\n",
    "        return loss\n",
    "\n",
    "model = L.Classifier(MLP(100, 10))\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (20, 'epoch'), out='result')\n",
    "\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/accuracy', 'validation/main/accuracy']))\n",
    "#trainer.extend(extensions.ProgressBar())\n",
    "trainer.run()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chainer.cuda.available True\n",
      "chainer.cuda.cudnn_enabled True\n",
      "epoch       main/accuracy  validation/main/accuracy\n",
      "\u001b[J1           0.802317       0.901                     \n",
      "\u001b[J2           0.903517       0.9146                    \n",
      "\u001b[J3           0.916201       0.9222                    \n",
      "\u001b[J4           0.924268       0.9274                    \n",
      "\u001b[J5           0.929767       0.9334                    \n",
      "\u001b[J6           0.935434       0.9368                    \n",
      "\u001b[J7           0.939851       0.9394                    \n",
      "\u001b[J8           0.943551       0.9432                    \n",
      "\u001b[J9           0.94735        0.9475                    \n",
      "\u001b[J10          0.950717       0.9472                    \n",
      "\u001b[J11          0.953783       0.9527                    \n",
      "\u001b[J12          0.95605        0.9539                    \n",
      "\u001b[J13          0.958801       0.9565                    \n",
      "\u001b[J14          0.9609         0.9569                    \n",
      "\u001b[J15          0.962883       0.9585                    \n",
      "\u001b[J16          0.965068       0.9604                    \n",
      "\u001b[J17          0.96715        0.9608                    \n",
      "\u001b[J18          0.968384       0.9613                    \n",
      "\u001b[J19          0.970149       0.9636                    \n",
      "\u001b[J20          0.971583       0.9636                    \n"
     ]
    }
   ],
   "source": [
    "# Run Neural Networks on a Single GPU\n",
    "\n",
    "print('chainer.cuda.available',chainer.cuda.available)\n",
    "print('chainer.cuda.cudnn_enabled',chainer.cuda.cudnn_enabled)\n",
    "\n",
    "train, test = datasets.get_mnist()\n",
    "train_iter = iterators.SerialIterator(train, batch_size=100, shuffle=True)\n",
    "test_iter = iterators.SerialIterator(test, batch_size=100, repeat=False, shuffle=False)\n",
    "\n",
    "class MLP(Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units).to_gpu()  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units).to_gpu()  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out).to_gpu()    # n_units -> n_out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.elu(self.l1(x))\n",
    "        h2 = F.elu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "\n",
    "class Classifier(Chain):\n",
    "    def __init__(self, predictor):\n",
    "        super(Classifier, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.predictor = predictor\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        accuracy = F.accuracy(y, t)\n",
    "        report({'loss': loss, 'accuracy': accuracy}, self)\n",
    "        return loss\n",
    "\n",
    "model = L.Classifier(MLP(100, 10))\n",
    "optimizer = optimizers.Adam(alpha=1e-4, beta1=0.9, beta2=0.999)\n",
    "optimizer.setup(model)\n",
    "\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=0)\n",
    "trainer = training.Trainer(updater, (20, 'epoch'), out='result')\n",
    "\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=0))\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/accuracy', 'validation/main/accuracy']))\n",
    "#trainer.extend(extensions.ProgressBar())\n",
    "trainer.run()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
